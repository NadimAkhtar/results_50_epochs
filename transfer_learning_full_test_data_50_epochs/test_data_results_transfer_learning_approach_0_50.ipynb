{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_data_results_transfer_learning_approach_0_50.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpQ8vA9rr9YR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ede7ed9-71ad-4497-dad2-bb3086f34f6f"
      },
      "source": [
        "! pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "! pip install seqeval\n",
        "! pip install sklearn_crfsuite\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from math import nan\n",
        "from future.utils import iteritems\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "import keras as k\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#loading train data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df=pd.read_csv('/content/drive//My Drive/entity_train.csv')\n",
        "\n",
        "df1=df[['type','Sentence ID','Text','Semantic Type']]\n",
        "\n",
        "df1=df1[df1['type']=='entity']  #We take only the entities i.e. removing the text and relation types\n",
        "\n",
        "df1=df1[['Sentence ID','Text','Semantic Type']]\n",
        "\n",
        "df1=df1.rename(columns={'Sentence ID':'sentence_idx','Text':'word','Semantic Type':'tag'})\n",
        "\n",
        "#loading test data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dftest=pd.read_csv('/content/drive//My Drive/entity_test.csv')\n",
        "\n",
        "dftest1=dftest[['type','Sentence ID','Text','Semantic Type']]\n",
        "\n",
        "dftest1=dftest1[dftest1['type']=='entity']  #We take only the entities i.e. removing the text and relation\n",
        "\n",
        "dftest1=dftest1[['Sentence ID','Text','Semantic Type']]\n",
        "\n",
        "dftest1=dftest1.rename(columns={'Sentence ID':'sentence_idx','Text':'word','Semantic Type':'tag'})\n",
        "\n",
        "###creating sentence id\n",
        "\n",
        "#for train data\n",
        "\n",
        "index_train=df1.index\n",
        "\n",
        "\n",
        "seq_train=[]\n",
        "seq_train.append(df1['sentence_idx'][index_train[0]])\n",
        "for i in range(1,len(index_train)):\n",
        "  seq_train.append(df1['sentence_idx'][index_train[i]]-df1['sentence_idx'][index_train[i-1]])\n",
        "len(seq_train)\n",
        "\n",
        "\n",
        "neg_ind_train=[]\n",
        "for i in range(len(seq_train)):\n",
        "  if seq_train[i]<0:\n",
        "    seq_train[i]=1\n",
        "    neg_ind_train.append(i)\n",
        "\n",
        "df1=df1.assign(ind_train=seq_train)\n",
        "sen_id=df1['ind_train'].cumsum()\n",
        "df1=df1.assign(sentence_idx=sen_id)\n",
        "df1=df1.drop('ind_train',1)\n",
        "df1=df1.dropna()\n",
        "\n",
        "#creating sentence id for test data\n",
        "\n",
        "index_test=dftest1.index\n",
        "\n",
        "\n",
        "seq_test=[]\n",
        "seq_test.append(dftest1['sentence_idx'][index_test[0]])\n",
        "for i in range(1,len(index_test)):\n",
        "  seq_test.append(dftest1['sentence_idx'][index_test[i]]-dftest1['sentence_idx'][index_test[i-1]])\n",
        "len(seq_test)\n",
        "\n",
        "\n",
        "neg_ind_test=[]\n",
        "for i in range(len(seq_test)):\n",
        "  if seq_test[i]<0:\n",
        "    seq_test[i]=1\n",
        "    neg_ind_test.append(i)\n",
        "\n",
        "dftest1=dftest1.assign(ind_test=seq_test)\n",
        "sen_id=dftest1['ind_test'].cumsum()\n",
        "dftest1=dftest1.assign(sentence_idx=sen_id)\n",
        "dftest1=dftest1.drop('ind_test',1)\n",
        "dftest1=dftest1.dropna()\n",
        "\n",
        "#Split test data into 2 half\n",
        "test_sp1, test_sp2 = train_test_split(dftest1, test_size=0.5,random_state=123)\n",
        "\n",
        "dftest_sp1=test_sp1.sort_index(axis = 0)\t# sort by index labels\n",
        "dfdev1, test_sp = train_test_split(test_sp2, test_size=0.5,random_state=123)\t#Split other half test data into dev and test data\n",
        "dfdev1=dfdev1.sort_index(axis=0)\n",
        "test_sp=test_sp.sort_index(axis=0)\n",
        "dftest1=test_sp\n",
        "\n",
        "#Taking only required tags and the rest renamed as others 'O'\n",
        "tag_req=['diap','fndg','lbpr','lbtr']\n",
        "df2=df1[df1.tag.str.contains('|'.join(tag_req))]\n",
        "\n",
        "req_train=df2.index\n",
        "\n",
        "for i in df1.index:\n",
        "  if i not in req_train:\n",
        "    df1.tag[i]='O'\n",
        "  else:\n",
        "    if ',' in df1.tag[i]:\n",
        "      df1.tag[i]='O'\n",
        "\n",
        "dftest2=dftest1[dftest1.tag.str.contains('|'.join(tag_req))]\n",
        "\n",
        "req_test=dftest2.index\n",
        "\n",
        "for i in dftest1.index:\n",
        "  if i not in req_test:\n",
        "    dftest1.tag[i]='O'\n",
        "  else:\n",
        "    if ',' in dftest1.tag[i]:\n",
        "      dftest1.tag[i]='O'\n",
        "\n",
        "dfdev2=dfdev1[dfdev1.tag.str.contains('|'.join(tag_req))]\n",
        "\n",
        "req_dev=dfdev2.index\n",
        "\n",
        "for i in dfdev1.index:\n",
        "  if i not in req_dev:\n",
        "    dfdev1.tag[i]='O'\n",
        "  else:\n",
        "    if ',' in dfdev1.tag[i]:\n",
        "      dfdev1.tag[i]='O'\n",
        "\n",
        "dftest_sp2=dftest_sp1[dftest_sp1.tag.str.contains('|'.join(tag_req))]\n",
        "\n",
        "req_test_sp=dftest_sp2.index\n",
        "\n",
        "for i in dftest_sp1.index:\n",
        "  if i not in req_test_sp:\n",
        "    dftest_sp1.tag[i]='O'\n",
        "  else:\n",
        "    if ',' in dftest_sp1.tag[i]:\n",
        "      dftest_sp1.tag[i]='O'\n",
        "\n",
        "#BIO-tagging For Train Data\n",
        "temp01=pd.DataFrame(df1.word.str.split().tolist(), index=df1['sentence_idx']).stack()\n",
        "d1 = temp01.index\n",
        "t1 = []\n",
        "for i in range(len(d1)):\n",
        "  if d1[i][1] == 0:\n",
        "    t1.append('B-')\n",
        "  else:\n",
        "    t1.append('I-')\n",
        "temp01 = temp01.reset_index()[[0, 'sentence_idx']] # 'word' variable is currently labeled 0\n",
        "temp01.columns = ['word','sentence_idx']\n",
        "temp01=temp01[['sentence_idx','word']]\n",
        "temp01=temp01.assign(bio_tr=t1)\n",
        "\n",
        "temp02=pd.DataFrame(df1.word.str.split().tolist(), index=df1['tag']).stack()\n",
        "temp02 = temp02.reset_index()[[0, 'tag']] # 'word' variable is currently labeled 0\n",
        "temp02.columns = ['word','tag']\n",
        "\n",
        "temp01[\"tag\"] = temp01[\"bio_tr\"].astype(str) + temp02[\"tag\"]\n",
        "del temp01['bio_tr']\n",
        "temp01['tag']=temp01['tag'].replace(['B-O','I-O'],'O')\n",
        "\n",
        "df1=temp01\n",
        "\n",
        "#BIO-tagging For Test Data\n",
        "temp_test01=pd.DataFrame(dftest1.word.str.split().tolist(), index=dftest1['sentence_idx']).stack()\n",
        "d1_test = temp_test01.index\n",
        "t1_test = []\n",
        "for i in range(len(d1_test)):\n",
        "  if d1_test[i][1] == 0:\n",
        "    t1_test.append('B-')\n",
        "  else:\n",
        "    t1_test.append('I-')\n",
        "temp_test01 = temp_test01.reset_index()[[0, 'sentence_idx']] # 'word' variable is currently labeled 0\n",
        "temp_test01.columns = ['word','sentence_idx']\n",
        "temp_test01=temp_test01[['sentence_idx','word']]\n",
        "temp_test01=temp_test01.assign(bio_te=t1_test)\n",
        "\n",
        "temp_test02=pd.DataFrame(dftest1.word.str.split().tolist(), index=dftest1['tag']).stack()\n",
        "temp_test02 = temp_test02.reset_index()[[0, 'tag']] # 'word' variable is currently labeled 0\n",
        "temp_test02.columns = ['word','tag']\n",
        "\n",
        "temp_test01[\"tag\"] = temp_test01[\"bio_te\"].astype(str) + temp_test02[\"tag\"]\n",
        "del temp_test01['bio_te']\n",
        "temp_test01['tag']=temp_test01['tag'].replace(['B-O','I-O'],'O')\n",
        "\n",
        "dftest1=temp_test01\n",
        "\n",
        "#BIO-tagging For dev Data\n",
        "temp_dev01=pd.DataFrame(dfdev1.word.str.split().tolist(), index=dfdev1['sentence_idx']).stack()\n",
        "d1_dev = temp_dev01.index\n",
        "t1_dev = []\n",
        "for i in range(len(d1_dev)):\n",
        "  if d1_dev[i][1] == 0:\n",
        "    t1_dev.append('B-')\n",
        "  else:\n",
        "    t1_dev.append('I-')\n",
        "temp_dev01 = temp_dev01.reset_index()[[0, 'sentence_idx']] # 'word' variable is currently labeled 0\n",
        "temp_dev01.columns = ['word','sentence_idx']\n",
        "temp_dev01=temp_dev01[['sentence_idx','word']]\n",
        "temp_dev01=temp_dev01.assign(bio_te=t1_dev)\n",
        "\n",
        "temp_dev02=pd.DataFrame(dfdev1.word.str.split().tolist(), index=dfdev1['tag']).stack()\n",
        "temp_dev02 = temp_dev02.reset_index()[[0, 'tag']] # 'word' variable is currently labeled 0\n",
        "temp_dev02.columns = ['word','tag']\n",
        "\n",
        "temp_dev01[\"tag\"] = temp_dev01[\"bio_te\"].astype(str) + temp_dev02[\"tag\"]\n",
        "del temp_dev01['bio_te']\n",
        "temp_dev01['tag']=temp_dev01['tag'].replace(['B-O','I-O'],'O')\n",
        "\n",
        "dfdev1=temp_dev01\n",
        "\n",
        "#BIO-tagging For test split1 Data\n",
        "temp_test_sp01=pd.DataFrame(dftest_sp1.word.str.split().tolist(), index=dftest_sp1['sentence_idx']).stack()\n",
        "d1_test_sp = temp_test_sp01.index\n",
        "t1_test_sp = []\n",
        "for i in range(len(d1_test_sp)):\n",
        "  if d1_test_sp[i][1] == 0:\n",
        "    t1_test_sp.append('B-')\n",
        "  else:\n",
        "    t1_test_sp.append('I-')\n",
        "temp_test_sp01 = temp_test_sp01.reset_index()[[0, 'sentence_idx']] # 'word' variable is currently labeled 0\n",
        "temp_test_sp01.columns = ['word','sentence_idx']\n",
        "temp_test_sp01=temp_test_sp01[['sentence_idx','word']]\n",
        "temp_test_sp01=temp_test_sp01.assign(bio_te=t1_test_sp)\n",
        "\n",
        "temp_test_sp02=pd.DataFrame(dftest_sp1.word.str.split().tolist(), index=dftest_sp1['tag']).stack()\n",
        "temp_test_sp02 = temp_test_sp02.reset_index()[[0, 'tag']] # 'word' variable is currently labeled 0\n",
        "temp_test_sp02.columns = ['word','tag']\n",
        "\n",
        "temp_test_sp01[\"tag\"] = temp_test_sp01[\"bio_te\"].astype(str) + temp_test_sp02[\"tag\"]\n",
        "del temp_test_sp01['bio_te']\n",
        "temp_test_sp01['tag']=temp_test_sp01['tag'].replace(['B-O','I-O'],'O')\n",
        "\n",
        "dftest_sp1=temp_test_sp01\n",
        "\n",
        "train=df1\n",
        "test=dftest1\n",
        "dev=dfdev1\n",
        "train2=dftest_sp1\n",
        "\n",
        "#Define Sentence Getter\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, dataset):\n",
        "        self.n_sent = 1\n",
        "        self.dataset = dataset\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
        "                                                        s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "#Sentence getter for train\n",
        "getter_train = SentenceGetter(train)\n",
        "sentences_train = getter_train.sentences\n",
        "\n",
        "#Sentence getter for test\n",
        "getter_test = SentenceGetter(test)\n",
        "sentences_test = getter_test.sentences\n",
        "\n",
        "#Sentence getter for dev\n",
        "getter_dev = SentenceGetter(dev)\n",
        "sentences_dev = getter_dev.sentences\n",
        "\n",
        "#Sentence getter for train2\n",
        "getter_train2 = SentenceGetter(train2)\n",
        "sentences_train2 = getter_train2.sentences\n",
        "\n",
        "##formation of words and tags\n",
        "\n",
        "#for train\n",
        "\n",
        "words_train = list(set(train[\"word\"].values))\n",
        "n_words_train = len(words_train)\n",
        "\n",
        "tags_train = []\n",
        "for tag in set(train[\"tag\"].values):\n",
        "    if tag is nan or isinstance(tag, float):\n",
        "        tags_train.append('unk')\n",
        "    else:\n",
        "        tags_train.append(tag)\n",
        "n_tags_train = len(tags_train)\n",
        "\n",
        "#for test\n",
        "words_test = list(set(test[\"word\"].values))\n",
        "n_words_test = len(words_test)\n",
        "\n",
        "tags_test = []\n",
        "for tag in set(test[\"tag\"].values):\n",
        "    if tag is nan or isinstance(tag, float):\n",
        "        tags_test.append('unk')\n",
        "    else:\n",
        "        tags_test.append(tag)\n",
        "n_tags_test = len(tags_test)\n",
        "\n",
        "#for dev\n",
        "words_dev = list(set(dev[\"word\"].values))\n",
        "n_words_dev = len(words_dev)\n",
        "\n",
        "tags_dev = []\n",
        "for tag in set(dev[\"tag\"].values):\n",
        "    if tag is nan or isinstance(tag, float):\n",
        "        tags_dev.append('unk')\n",
        "    else:\n",
        "        tags_dev.append(tag)\n",
        "n_tags_dev = len(tags_dev)\n",
        "\n",
        "#for train2\n",
        "words_train2 = list(set(train2[\"word\"].values))\n",
        "n_words_train2 = len(words_train2)\n",
        "\n",
        "tags_train2 = []\n",
        "for tag in set(train2[\"tag\"].values):\n",
        "    if tag is nan or isinstance(tag, float):\n",
        "        tags_train2.append('unk')\n",
        "    else:\n",
        "        tags_train2.append(tag)\n",
        "n_tags_train2 = len(tags_train2)\n",
        "\n",
        "#taking union of train, dev and test\n",
        "\n",
        "words_all = list(set().union(words_train,words_test,words_dev,words_train2))\n",
        "n_words_all = len(words_all)\n",
        "\n",
        "tags_all = list(set().union(tags_train,tags_test,tags_dev,tags_train2))\n",
        "n_tags_all = len(tags_all)\n",
        "\n",
        "##formation of word2id, tag2id and id2tag\n",
        "\n",
        "#for all union of train and test\n",
        "word2idx_all = {w: i for i, w in enumerate(words_all)}\n",
        "tag2idx_all = {t: i for i, t in enumerate(tags_all)}\n",
        "idx2tag_all = {v: k for k, v in iteritems(tag2idx_all)}\n",
        "\n",
        "maxlen_all = max(max([len(s) for s in sentences_train]),max([len(s) for s in sentences_test]),max([len(s) for s in sentences_dev]),max([len(s) for s in sentences_train2]))\n",
        "\n",
        "##vectorisation\n",
        "\n",
        "#for train\n",
        "\n",
        "maxlen_train = max([len(s) for s in sentences_train])\n",
        "\n",
        "X_train = [[word2idx_all[w[0]] for w in s] for s in sentences_train]\n",
        "X_train = pad_sequences(maxlen=maxlen_all, sequences=X_train, padding=\"post\",value=n_words_all - 1)\n",
        "\n",
        "y_train = [[tag2idx_all[w[1]] for w in s] for s in sentences_train]\n",
        "y_train = pad_sequences(maxlen=maxlen_all, sequences=y_train, padding=\"post\", value=tag2idx_all[\"O\"])\n",
        "y_train = [to_categorical(i, num_classes=n_tags_all) for i in y_train]\n",
        "\n",
        "\n",
        "#for test\n",
        "maxlen_test = max([len(s) for s in sentences_test])\n",
        "\n",
        "X_test = [[word2idx_all[w[0]] for w in s] for s in sentences_test]\n",
        "X_test = pad_sequences(maxlen=maxlen_all, sequences=X_test, padding=\"post\",value=n_words_all - 1)\n",
        "\n",
        "y_test = [[tag2idx_all[w[1]] for w in s] for s in sentences_test]\n",
        "y_test = pad_sequences(maxlen=maxlen_all, sequences=y_test, padding=\"post\", value=tag2idx_all[\"O\"])\n",
        "y_test = [to_categorical(i, num_classes=n_tags_all) for i in y_test]\n",
        "\n",
        "#for dev\n",
        "maxlen_dev = max([len(s) for s in sentences_dev])\n",
        "\n",
        "X_dev = [[word2idx_all[w[0]] for w in s] for s in sentences_dev]\n",
        "X_dev = pad_sequences(maxlen=maxlen_all, sequences=X_dev, padding=\"post\",value=n_words_all - 1)\n",
        "\n",
        "y_dev = [[tag2idx_all[w[1]] for w in s] for s in sentences_dev]\n",
        "y_dev = pad_sequences(maxlen=maxlen_all, sequences=y_dev, padding=\"post\", value=tag2idx_all[\"O\"])\n",
        "y_dev = [to_categorical(i, num_classes=n_tags_all) for i in y_dev]\n",
        "\n",
        "#for train2\n",
        "maxlen_train2 = max([len(s) for s in sentences_train2])\n",
        "\n",
        "X_train2 = [[word2idx_all[w[0]] for w in s] for s in sentences_train2]\n",
        "X_train2 = pad_sequences(maxlen=maxlen_all, sequences=X_train2, padding=\"post\",value=n_words_all - 1)\n",
        "\n",
        "y_train2 = [[tag2idx_all[w[1]] for w in s] for s in sentences_train2]\n",
        "y_train2 = pad_sequences(maxlen=maxlen_all, sequences=y_train2, padding=\"post\", value=tag2idx_all[\"O\"])\n",
        "y_train2 = [to_categorical(i, num_classes=n_tags_all) for i in y_train2]\n",
        "\n",
        "##MODEL\n",
        "\n",
        "input = Input(shape=(max(X_train.shape[1],X_dev.shape[1],X_test.shape[1],X_train2.shape[1]),))\n",
        "word_embedding_size = 180\n",
        "\n",
        "# Embedding Layer\n",
        "model = Embedding(input_dim=n_words_all, output_dim=word_embedding_size, input_length=max(X_train.shape[1],X_dev.shape[1],X_test.shape[1],X_train2.shape[1]))(input)\n",
        "\n",
        "# BI-LSTM Layer\n",
        "model = Bidirectional(LSTM(units=word_embedding_size, \n",
        "                           return_sequences=True, \n",
        "                           dropout=0.5, \n",
        "                           recurrent_dropout=0.5, \n",
        "                           kernel_initializer=k.initializers.he_normal()))(model)\n",
        "model = LSTM(units=word_embedding_size * 2, \n",
        "             return_sequences=True, \n",
        "             dropout=0.5, \n",
        "             recurrent_dropout=0.5, \n",
        "             kernel_initializer=k.initializers.he_normal())(model)\n",
        "\n",
        "# TimeDistributed Layer\n",
        "model = TimeDistributed(Dense(n_tags_all, activation=\"relu\"))(model)  \n",
        "\n",
        "# CRF Layer\n",
        "crf = CRF(n_tags_all)\n",
        "\n",
        "out = crf(model)  # output\n",
        "model = Model(input, out)\n",
        "\n",
        "##FIT MODEL\n",
        "\n",
        "#Optimiser \n",
        "adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Saving the best model only\n",
        "filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-m8jhyxjr\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-m8jhyxjr\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101064 sha256=835a63b2e832be6b2a916fdecf6be033e96c37f6e4a7b91158feb5d68147b592\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q_yxdhe2/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=ce2e4ad403253dd4dc1e06557da8b650d02d6c295720b5e5bc9a7997dde38db3\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.12.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.7)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.41.1)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (18,19,20,24,25,32,33,47,48,49,50,51,52,53,54,60,61,67,68) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (23,32,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:125: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:147: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:150: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 274)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 274, 180)          3860640   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 274, 360)          519840    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 274, 360)          1038240   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 274, 9)            3249      \n",
            "_________________________________________________________________\n",
            "crf_1 (CRF)                  (None, 274, 9)            189       \n",
            "=================================================================\n",
            "Total params: 5,422,158\n",
            "Trainable params: 5,422,158\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8bE7xbGs04C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9c11740-0d5e-4a86-a780-142da037c2b9"
      },
      "source": [
        "# Fit the best model with train data\n",
        "history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=50, validation_split=0.1, verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 44782 samples, validate on 4976 samples\n",
            "Epoch 1/50\n",
            "44782/44782 [==============================] - 344s 8ms/step - loss: 0.0854 - crf_viterbi_accuracy: 0.9641 - accuracy: 1.0652e-04 - val_loss: 0.0195 - val_crf_viterbi_accuracy: 0.9968 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.99686, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 2/50\n",
            "44782/44782 [==============================] - 344s 8ms/step - loss: 0.0171 - crf_viterbi_accuracy: 0.9970 - accuracy: 1.0652e-04 - val_loss: 0.0156 - val_crf_viterbi_accuracy: 0.9968 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.99686\n",
            "Epoch 3/50\n",
            "44782/44782 [==============================] - 340s 8ms/step - loss: 0.0137 - crf_viterbi_accuracy: 0.9970 - accuracy: 1.0652e-04 - val_loss: 0.0114 - val_crf_viterbi_accuracy: 0.9968 - val_accuracy: 0.9969\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.99686\n",
            "Epoch 4/50\n",
            "44782/44782 [==============================] - 340s 8ms/step - loss: 0.0091 - crf_viterbi_accuracy: 0.9972 - accuracy: 1.0652e-04 - val_loss: 0.0069 - val_crf_viterbi_accuracy: 0.9976 - val_accuracy: 0.9976\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.99686 to 0.99760, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 5/50\n",
            "44782/44782 [==============================] - 341s 8ms/step - loss: 0.0060 - crf_viterbi_accuracy: 0.9978 - accuracy: 1.0652e-04 - val_loss: 0.0052 - val_crf_viterbi_accuracy: 0.9979 - val_accuracy: 0.9979\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.99760 to 0.99790, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 6/50\n",
            "44782/44782 [==============================] - 340s 8ms/step - loss: 0.0045 - crf_viterbi_accuracy: 0.9980 - accuracy: 1.0652e-04 - val_loss: 0.0042 - val_crf_viterbi_accuracy: 0.9981 - val_accuracy: 0.9981\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.99790 to 0.99809, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 7/50\n",
            "44782/44782 [==============================] - 333s 7ms/step - loss: 0.0034 - crf_viterbi_accuracy: 0.9982 - accuracy: 1.0652e-04 - val_loss: 0.0033 - val_crf_viterbi_accuracy: 0.9983 - val_accuracy: 0.9983\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.99809 to 0.99827, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 8/50\n",
            "44782/44782 [==============================] - 337s 8ms/step - loss: 0.0023 - crf_viterbi_accuracy: 0.9984 - accuracy: 1.0652e-04 - val_loss: 0.0023 - val_crf_viterbi_accuracy: 0.9985 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.99827 to 0.99846, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 9/50\n",
            "44782/44782 [==============================] - 351s 8ms/step - loss: 0.0013 - crf_viterbi_accuracy: 0.9986 - accuracy: 1.0652e-04 - val_loss: 0.0016 - val_crf_viterbi_accuracy: 0.9985 - val_accuracy: 0.9985\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.99846 to 0.99852, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 10/50\n",
            "44782/44782 [==============================] - 356s 8ms/step - loss: 5.1048e-04 - crf_viterbi_accuracy: 0.9987 - accuracy: 1.0652e-04 - val_loss: 9.6593e-04 - val_crf_viterbi_accuracy: 0.9986 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.99852 to 0.99858, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 11/50\n",
            "44782/44782 [==============================] - 336s 8ms/step - loss: -1.9994e-04 - crf_viterbi_accuracy: 0.9988 - accuracy: 1.0652e-04 - val_loss: 3.8599e-04 - val_crf_viterbi_accuracy: 0.9986 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.99858 to 0.99864, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 12/50\n",
            "44782/44782 [==============================] - 351s 8ms/step - loss: -8.3855e-04 - crf_viterbi_accuracy: 0.9989 - accuracy: 1.0652e-04 - val_loss: -8.1742e-05 - val_crf_viterbi_accuracy: 0.9986 - val_accuracy: 0.9986\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99864\n",
            "Epoch 13/50\n",
            "44782/44782 [==============================] - 340s 8ms/step - loss: -0.0014 - crf_viterbi_accuracy: 0.9989 - accuracy: 1.0652e-04 - val_loss: -4.6978e-04 - val_crf_viterbi_accuracy: 0.9987 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.99864 to 0.99868, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 14/50\n",
            "44782/44782 [==============================] - 331s 7ms/step - loss: -0.0019 - crf_viterbi_accuracy: 0.9989 - accuracy: 1.0652e-04 - val_loss: -9.7687e-04 - val_crf_viterbi_accuracy: 0.9987 - val_accuracy: 0.9987\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.99868 to 0.99873, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 15/50\n",
            "44782/44782 [==============================] - 333s 7ms/step - loss: -0.0024 - crf_viterbi_accuracy: 0.9990 - accuracy: 1.0652e-04 - val_loss: -0.0014 - val_crf_viterbi_accuracy: 0.9988 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.99873 to 0.99876, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 16/50\n",
            "44782/44782 [==============================] - 355s 8ms/step - loss: -0.0029 - crf_viterbi_accuracy: 0.9990 - accuracy: 1.0652e-04 - val_loss: -0.0018 - val_crf_viterbi_accuracy: 0.9988 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.99876 to 0.99877, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 17/50\n",
            "44782/44782 [==============================] - 340s 8ms/step - loss: -0.0034 - crf_viterbi_accuracy: 0.9991 - accuracy: 1.0652e-04 - val_loss: -0.0022 - val_crf_viterbi_accuracy: 0.9988 - val_accuracy: 0.9988\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.99877 to 0.99878, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 18/50\n",
            "44782/44782 [==============================] - 331s 7ms/step - loss: -0.0038 - crf_viterbi_accuracy: 0.9991 - accuracy: 1.0652e-04 - val_loss: -0.0025 - val_crf_viterbi_accuracy: 0.9988 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.99878 to 0.99885, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 19/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0043 - crf_viterbi_accuracy: 0.9992 - accuracy: 1.0652e-04 - val_loss: -0.0028 - val_crf_viterbi_accuracy: 0.9989 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.99885 to 0.99891, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 20/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0048 - crf_viterbi_accuracy: 0.9992 - accuracy: 1.0652e-04 - val_loss: -0.0033 - val_crf_viterbi_accuracy: 0.9989 - val_accuracy: 0.9989\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.99891 to 0.99893, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 21/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0052 - crf_viterbi_accuracy: 0.9993 - accuracy: 1.0652e-04 - val_loss: -0.0036 - val_crf_viterbi_accuracy: 0.9990 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.99893 to 0.99896, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 22/50\n",
            "44782/44782 [==============================] - 331s 7ms/step - loss: -0.0056 - crf_viterbi_accuracy: 0.9994 - accuracy: 1.0652e-04 - val_loss: -0.0041 - val_crf_viterbi_accuracy: 0.9990 - val_accuracy: 0.9990\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.99896 to 0.99903, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 23/50\n",
            "44782/44782 [==============================] - 335s 7ms/step - loss: -0.0061 - crf_viterbi_accuracy: 0.9995 - accuracy: 1.0652e-04 - val_loss: -0.0044 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.99903 to 0.99913, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 24/50\n",
            "44782/44782 [==============================] - 331s 7ms/step - loss: -0.0066 - crf_viterbi_accuracy: 0.9995 - accuracy: 1.0652e-04 - val_loss: -0.0048 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.99913\n",
            "Epoch 25/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0070 - crf_viterbi_accuracy: 0.9995 - accuracy: 1.0652e-04 - val_loss: -0.0051 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.99913 to 0.99914, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 26/50\n",
            "44782/44782 [==============================] - 331s 7ms/step - loss: -0.0074 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.0652e-04 - val_loss: -0.0055 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.99914 to 0.99916, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 27/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0078 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.0652e-04 - val_loss: -0.0058 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.99916 to 0.99918, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 28/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0082 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.0652e-04 - val_loss: -0.0062 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.99918\n",
            "Epoch 29/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0085 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.0652e-04 - val_loss: -0.0065 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.99918 to 0.99920, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 30/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0089 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.0652e-04 - val_loss: -0.0067 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.99920\n",
            "Epoch 31/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0093 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.0652e-04 - val_loss: -0.0070 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.99920\n",
            "Epoch 32/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0097 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0074 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.99920\n",
            "Epoch 33/50\n",
            "44782/44782 [==============================] - 331s 7ms/step - loss: -0.0100 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0077 - val_crf_viterbi_accuracy: 0.9991 - val_accuracy: 0.9991\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.99920\n",
            "Epoch 34/50\n",
            "44782/44782 [==============================] - 331s 7ms/step - loss: -0.0104 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0081 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.99920\n",
            "Epoch 35/50\n",
            "44782/44782 [==============================] - 332s 7ms/step - loss: -0.0107 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0083 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.99920\n",
            "Epoch 36/50\n",
            "44782/44782 [==============================] - 332s 7ms/step - loss: -0.0111 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0087 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.99920\n",
            "Epoch 37/50\n",
            "44782/44782 [==============================] - 332s 7ms/step - loss: -0.0114 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0090 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.99920\n",
            "Epoch 38/50\n",
            "44782/44782 [==============================] - 348s 8ms/step - loss: -0.0118 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0094 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.99920\n",
            "Epoch 39/50\n",
            "44782/44782 [==============================] - 340s 8ms/step - loss: -0.0121 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.0652e-04 - val_loss: -0.0097 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.99920\n",
            "Epoch 40/50\n",
            "44782/44782 [==============================] - 343s 8ms/step - loss: -0.0125 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0098 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.99920\n",
            "Epoch 41/50\n",
            "44782/44782 [==============================] - 340s 8ms/step - loss: -0.0128 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0103 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.99920 to 0.99921, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 42/50\n",
            "44782/44782 [==============================] - 334s 7ms/step - loss: -0.0132 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0106 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.99921\n",
            "Epoch 43/50\n",
            "44782/44782 [==============================] - 328s 7ms/step - loss: -0.0135 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0110 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.99921\n",
            "Epoch 44/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0139 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0111 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.99921 to 0.99922, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 45/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0142 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0114 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.99922\n",
            "Epoch 46/50\n",
            "44782/44782 [==============================] - 327s 7ms/step - loss: -0.0146 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0118 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.99922\n",
            "Epoch 47/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0149 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0121 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.99922 to 0.99922, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 48/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0152 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0124 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.99922\n",
            "Epoch 49/50\n",
            "44782/44782 [==============================] - 329s 7ms/step - loss: -0.0156 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0126 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00049: val_accuracy improved from 0.99922 to 0.99922, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 50/50\n",
            "44782/44782 [==============================] - 330s 7ms/step - loss: -0.0159 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.0652e-04 - val_loss: -0.0130 - val_crf_viterbi_accuracy: 0.9992 - val_accuracy: 0.9992\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.99922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN1X3xxQs38F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f9dcc58-78d7-47ca-8cd9-6c7d99dcb2a2"
      },
      "source": [
        "# Fit the best model with train2 data\n",
        "history = model.fit(X_train2, np.array(y_train2), batch_size=256, epochs=50, validation_data=(X_dev, np.array(y_dev)), verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 27810 samples, validate on 20655 samples\n",
            "Epoch 1/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0142 - crf_viterbi_accuracy: 0.9993 - accuracy: 1.2218e-04 - val_loss: -0.0156 - val_crf_viterbi_accuracy: 0.9996 - val_accuracy: 0.9996\n",
            "\n",
            "Epoch 00001: val_accuracy improved from 0.99922 to 0.99964, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 2/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0153 - crf_viterbi_accuracy: 0.9995 - accuracy: 1.2218e-04 - val_loss: -0.0158 - val_crf_viterbi_accuracy: 0.9996 - val_accuracy: 0.9996\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.99964 to 0.99965, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 3/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0158 - crf_viterbi_accuracy: 0.9995 - accuracy: 1.2218e-04 - val_loss: -0.0161 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.99965 to 0.99966, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 4/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0162 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.2218e-04 - val_loss: -0.0163 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.99966 to 0.99967, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 5/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0165 - crf_viterbi_accuracy: 0.9996 - accuracy: 1.2218e-04 - val_loss: -0.0165 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.99967\n",
            "Epoch 6/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0168 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.2218e-04 - val_loss: -0.0167 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.99967 to 0.99969, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 7/50\n",
            "27810/27810 [==============================] - 226s 8ms/step - loss: -0.0171 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.2218e-04 - val_loss: -0.0169 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99969\n",
            "Epoch 8/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0174 - crf_viterbi_accuracy: 0.9997 - accuracy: 1.2218e-04 - val_loss: -0.0171 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.99969\n",
            "Epoch 9/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0176 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.2218e-04 - val_loss: -0.0173 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99969\n",
            "Epoch 10/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0179 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.2218e-04 - val_loss: -0.0174 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99969\n",
            "Epoch 11/50\n",
            "27810/27810 [==============================] - 226s 8ms/step - loss: -0.0181 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.2218e-04 - val_loss: -0.0176 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.99969\n",
            "Epoch 12/50\n",
            "27810/27810 [==============================] - 226s 8ms/step - loss: -0.0184 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.2218e-04 - val_loss: -0.0178 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99969\n",
            "Epoch 13/50\n",
            "27810/27810 [==============================] - 226s 8ms/step - loss: -0.0186 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.2218e-04 - val_loss: -0.0180 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.99969\n",
            "Epoch 14/50\n",
            "27810/27810 [==============================] - 227s 8ms/step - loss: -0.0188 - crf_viterbi_accuracy: 0.9998 - accuracy: 1.2218e-04 - val_loss: -0.0181 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.99969\n",
            "Epoch 15/50\n",
            "27810/27810 [==============================] - 226s 8ms/step - loss: -0.0190 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0183 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.99969\n",
            "Epoch 16/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0193 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0185 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.99969\n",
            "Epoch 17/50\n",
            "27810/27810 [==============================] - 226s 8ms/step - loss: -0.0195 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0186 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.99969\n",
            "Epoch 18/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0197 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0188 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.99969\n",
            "Epoch 19/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0199 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0190 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.99969\n",
            "Epoch 20/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0201 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0192 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.99969\n",
            "Epoch 21/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0203 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0194 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.99969\n",
            "Epoch 22/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0205 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0196 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.99969\n",
            "Epoch 23/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0207 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0197 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.99969\n",
            "Epoch 24/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0210 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0200 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.99969\n",
            "Epoch 25/50\n",
            "27810/27810 [==============================] - 226s 8ms/step - loss: -0.0211 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0201 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.99969\n",
            "Epoch 26/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0214 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0203 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.99969\n",
            "Epoch 27/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0216 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0205 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.99969\n",
            "Epoch 28/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0218 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0207 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.99969\n",
            "Epoch 29/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0220 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0209 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.99969\n",
            "Epoch 30/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0222 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0210 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.99969\n",
            "Epoch 31/50\n",
            "27810/27810 [==============================] - 223s 8ms/step - loss: -0.0224 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0212 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.99969\n",
            "Epoch 32/50\n",
            "27810/27810 [==============================] - 223s 8ms/step - loss: -0.0226 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0214 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.99969\n",
            "Epoch 33/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0228 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0216 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.99969\n",
            "Epoch 34/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0230 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0218 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.99969\n",
            "Epoch 35/50\n",
            "27810/27810 [==============================] - 223s 8ms/step - loss: -0.0232 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0220 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.99969\n",
            "Epoch 36/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0234 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0222 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.99969\n",
            "Epoch 37/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0236 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0223 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.99969\n",
            "Epoch 38/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0238 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0226 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.99969\n",
            "Epoch 39/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0240 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0228 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.99969\n",
            "Epoch 40/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0242 - crf_viterbi_accuracy: 1.0000 - accuracy: 1.2218e-04 - val_loss: -0.0230 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.99969\n",
            "Epoch 41/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0244 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0231 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.99969\n",
            "Epoch 42/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0246 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0233 - val_crf_viterbi_accuracy: 0.9996 - val_accuracy: 0.9996\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.99969\n",
            "Epoch 43/50\n",
            "27810/27810 [==============================] - 223s 8ms/step - loss: -0.0248 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0235 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.99969 to 0.99969, saving model to ner-bi-lstm-td-model-1.00.hdf5\n",
            "Epoch 44/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0250 - crf_viterbi_accuracy: 0.9999 - accuracy: 1.2218e-04 - val_loss: -0.0237 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.99969\n",
            "Epoch 45/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0252 - crf_viterbi_accuracy: 1.0000 - accuracy: 1.2218e-04 - val_loss: -0.0239 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.99969\n",
            "Epoch 46/50\n",
            "27810/27810 [==============================] - 224s 8ms/step - loss: -0.0254 - crf_viterbi_accuracy: 1.0000 - accuracy: 1.2218e-04 - val_loss: -0.0241 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.99969\n",
            "Epoch 47/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0256 - crf_viterbi_accuracy: 1.0000 - accuracy: 1.2218e-04 - val_loss: -0.0243 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.99969\n",
            "Epoch 48/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0259 - crf_viterbi_accuracy: 1.0000 - accuracy: 1.2218e-04 - val_loss: -0.0245 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.99969\n",
            "Epoch 49/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0261 - crf_viterbi_accuracy: 1.0000 - accuracy: 1.2218e-04 - val_loss: -0.0247 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.99969\n",
            "Epoch 50/50\n",
            "27810/27810 [==============================] - 225s 8ms/step - loss: -0.0262 - crf_viterbi_accuracy: 1.0000 - accuracy: 1.2218e-04 - val_loss: -0.0249 - val_crf_viterbi_accuracy: 0.9997 - val_accuracy: 0.9997\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.99969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZXhFPhcs34O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7d00d87-62b4-47b9-90bc-6a32ed92ccbf"
      },
      "source": [
        "####PLOTS of loss and accuracy\n",
        "# Plot the graph \n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(accuracy) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, accuracy, 'b', label='Training acc')\n",
        "    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "####FIT with the TEST data\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag_all[p_i])\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "test_pred = model.predict(X_test, verbose=1)   \n",
        "pred_labels = pred2label(test_pred)\n",
        "test_labels = pred2label(y_test)\n",
        "\n",
        "#####REPORT of the fit\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))\n",
        "\n",
        "report = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\n",
        "print(report)\n",
        "\n",
        "###Rest\n",
        "TP = {}\n",
        "TN = {}\n",
        "FP = {}\n",
        "FN = {}\n",
        "for tag in tag2idx_all.keys():\n",
        "    TP[tag] = 0\n",
        "    TN[tag] = 0    \n",
        "    FP[tag] = 0    \n",
        "    FN[tag] = 0    \n",
        "\n",
        "def accumulate_score_by_tag(gt, pred):\n",
        "    \"\"\"\n",
        "    For each tag keep stats\n",
        "    \"\"\"\n",
        "    if gt == pred:\n",
        "        TP[gt] += 1\n",
        "    elif gt != 'O' and pred == 'O':\n",
        "        FN[gt] +=1\n",
        "    elif gt == 'O' and pred != 'O':\n",
        "        FP[gt] += 1\n",
        "    else:\n",
        "        TN[gt] += 1\n",
        "\n",
        "for i, sentence in enumerate(X_test):\n",
        "    y_hat = np.argmax(test_pred[0], axis=-1)\n",
        "    gt = np.argmax(y_test[0], axis=-1)\n",
        "    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n",
        "        accumulate_score_by_tag(idx2tag_all[gt[idx]],tags_all[pred])\n",
        "\n",
        "for tag in tag2idx_all.keys():\n",
        "    print(f'tag:{tag}')    \n",
        "    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n",
        "    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20625/20625 [==============================] - 124s 6ms/step\n",
            "F1-score: 87.0%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      B-diap       0.92      0.90      0.91       878\n",
            "      B-fndg       0.88      0.87      0.88      2971\n",
            "      B-lbpr       0.91      0.92      0.92       443\n",
            "      B-lbtr       0.80      0.56      0.66        59\n",
            "      I-diap       0.89      0.87      0.88       470\n",
            "      I-fndg       0.83      0.82      0.82      2520\n",
            "      I-lbpr       0.79      0.84      0.81       208\n",
            "      I-lbtr       0.91      0.40      0.56        72\n",
            "           O       1.00      1.00      1.00   5643629\n",
            "\n",
            "    accuracy                           1.00   5651250\n",
            "   macro avg       0.88      0.80      0.83   5651250\n",
            "weighted avg       1.00      1.00      1.00   5651250\n",
            "\n",
            "tag:I-diap\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:         0\n",
            "tag:O\n",
            "\t TN:         0\tFP:     41250\n",
            "\t FN:         0\tTP:   5568750\n",
            "tag:I-fndg\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:     20625\n",
            "tag:I-lbtr\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:         0\n",
            "tag:B-lbtr\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:         0\n",
            "tag:I-lbpr\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:         0\n",
            "tag:B-diap\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:         0\n",
            "tag:B-fndg\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:     20625\n",
            "tag:B-lbpr\n",
            "\t TN:         0\tFP:         0\n",
            "\t FN:         0\tTP:         0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwcp6Qo2s31o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "e2b96358-7805-4762-a0ff-42d1967e23a1"
      },
      "source": [
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence ID</th>\n",
              "      <th>type</th>\n",
              "      <th>CUI</th>\n",
              "      <th>Preferred name</th>\n",
              "      <th>Semantic Type</th>\n",
              "      <th>Text</th>\n",
              "      <th>Confidence Score</th>\n",
              "      <th>First char pos</th>\n",
              "      <th>Last char pos</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "      <th>Unnamed: 13</th>\n",
              "      <th>Unnamed: 14</th>\n",
              "      <th>Unnamed: 15</th>\n",
              "      <th>Unnamed: 16</th>\n",
              "      <th>Unnamed: 17</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "      <th>Unnamed: 19</th>\n",
              "      <th>Unnamed: 20</th>\n",
              "      <th>Unnamed: 21</th>\n",
              "      <th>Unnamed: 22</th>\n",
              "      <th>Unnamed: 23</th>\n",
              "      <th>Unnamed: 24</th>\n",
              "      <th>Unnamed: 25</th>\n",
              "      <th>Unnamed: 26</th>\n",
              "      <th>Unnamed: 27</th>\n",
              "      <th>Unnamed: 28</th>\n",
              "      <th>Unnamed: 29</th>\n",
              "      <th>Unnamed: 30</th>\n",
              "      <th>Unnamed: 31</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "      <th>Unnamed: 33</th>\n",
              "      <th>Unnamed: 34</th>\n",
              "      <th>Unnamed: 35</th>\n",
              "      <th>Unnamed: 36</th>\n",
              "      <th>Unnamed: 37</th>\n",
              "      <th>Unnamed: 38</th>\n",
              "      <th>Unnamed: 39</th>\n",
              "      <th>Unnamed: 40</th>\n",
              "      <th>Unnamed: 41</th>\n",
              "      <th>Unnamed: 42</th>\n",
              "      <th>Unnamed: 43</th>\n",
              "      <th>Unnamed: 44</th>\n",
              "      <th>Unnamed: 45</th>\n",
              "      <th>Unnamed: 46</th>\n",
              "      <th>Unnamed: 47</th>\n",
              "      <th>Unnamed: 48</th>\n",
              "      <th>Unnamed: 49</th>\n",
              "      <th>Unnamed: 50</th>\n",
              "      <th>Unnamed: 51</th>\n",
              "      <th>Unnamed: 52</th>\n",
              "      <th>Unnamed: 53</th>\n",
              "      <th>Unnamed: 54</th>\n",
              "      <th>Unnamed: 55</th>\n",
              "      <th>Unnamed: 56</th>\n",
              "      <th>Unnamed: 57</th>\n",
              "      <th>Unnamed: 58</th>\n",
              "      <th>Unnamed: 59</th>\n",
              "      <th>Unnamed: 60</th>\n",
              "      <th>Unnamed: 61</th>\n",
              "      <th>Unnamed: 62</th>\n",
              "      <th>Unnamed: 63</th>\n",
              "      <th>Unnamed: 64</th>\n",
              "      <th>Unnamed: 65</th>\n",
              "      <th>Unnamed: 66</th>\n",
              "      <th>Unnamed: 67</th>\n",
              "      <th>Unnamed: 68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>text</td>\n",
              "      <td>0</td>\n",
              "      <td>279</td>\n",
              "      <td>RECORD #106886 192629344</td>\n",
              "      <td></td>\n",
              "      <td>Signed</td>\n",
              "      <td>DIS</td>\n",
              "      <td>Admission Date: 7/10/2006 Report Status: Sign...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>entity</td>\n",
              "      <td>C0034869</td>\n",
              "      <td>Records</td>\n",
              "      <td>inpr</td>\n",
              "      <td>RECORD</td>\n",
              "      <td>754</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>entity</td>\n",
              "      <td>C3843787</td>\n",
              "      <td>10-Dec</td>\n",
              "      <td>fndg</td>\n",
              "      <td>10/2006 12</td>\n",
              "      <td>754</td>\n",
              "      <td>57</td>\n",
              "      <td>67</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>entity</td>\n",
              "      <td>C0010054</td>\n",
              "      <td>Coronary Arteriosclerosis</td>\n",
              "      <td>dsyn</td>\n",
              "      <td>CORONARY ARTERY DISEASE</td>\n",
              "      <td>622</td>\n",
              "      <td>79</td>\n",
              "      <td>102</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>entity</td>\n",
              "      <td>C1519316</td>\n",
              "      <td>Signature</td>\n",
              "      <td>inpr</td>\n",
              "      <td>Signed</td>\n",
              "      <td>622</td>\n",
              "      <td>105</td>\n",
              "      <td>111</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397348</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397349</th>\n",
              "      <td>149.0</td>\n",
              "      <td>text</td>\n",
              "      <td>16423</td>\n",
              "      <td>16583</td>\n",
              "      <td>eScription document: 4-6031519 EMSSten Tel Dic...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397350</th>\n",
              "      <td>149.0</td>\n",
              "      <td>entity</td>\n",
              "      <td>C1301746</td>\n",
              "      <td>Documents</td>\n",
              "      <td>inpr</td>\n",
              "      <td>document</td>\n",
              "      <td>861</td>\n",
              "      <td>16434</td>\n",
              "      <td>16442</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397351</th>\n",
              "      <td>149.0</td>\n",
              "      <td>entity</td>\n",
              "      <td>C0796520</td>\n",
              "      <td>ETV6 gene</td>\n",
              "      <td>gngm</td>\n",
              "      <td>Tel</td>\n",
              "      <td>637</td>\n",
              "      <td>16462</td>\n",
              "      <td>16465</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397352</th>\n",
              "      <td>149.0</td>\n",
              "      <td>entity</td>\n",
              "      <td>C0444930</td>\n",
              "      <td>End</td>\n",
              "      <td>spco</td>\n",
              "      <td>end</td>\n",
              "      <td>888</td>\n",
              "      <td>16579</td>\n",
              "      <td>16582</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>397353 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Sentence ID    type       CUI  ... Unnamed: 66 Unnamed: 67 Unnamed: 68\n",
              "0               1.0    text         0  ...         NaN         NaN         NaN\n",
              "1               1.0  entity  C0034869  ...         NaN         NaN         NaN\n",
              "2               1.0  entity  C3843787  ...         NaN         NaN         NaN\n",
              "3               1.0  entity  C0010054  ...         NaN         NaN         NaN\n",
              "4               1.0  entity  C1519316  ...         NaN         NaN         NaN\n",
              "...             ...     ...       ...  ...         ...         ...         ...\n",
              "397348          NaN     NaN       NaN  ...         NaN         NaN         NaN\n",
              "397349        149.0    text     16423  ...         NaN         NaN         NaN\n",
              "397350        149.0  entity  C1301746  ...         NaN         NaN         NaN\n",
              "397351        149.0  entity  C0796520  ...         NaN         NaN         NaN\n",
              "397352        149.0  entity  C0444930  ...         NaN         NaN         NaN\n",
              "\n",
              "[397353 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmTyY3zQao6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "b9686ed9-6702-4367-85e8-4f90b947e457"
      },
      "source": [
        "train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>RECORD</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>10/2006</td>\n",
              "      <td>B-fndg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>12</td>\n",
              "      <td>I-fndg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>CORONARY</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>ARTERY</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350585</th>\n",
              "      <td>55259.0</td>\n",
              "      <td>outpatient</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350586</th>\n",
              "      <td>55259.0</td>\n",
              "      <td>cardiologist</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350587</th>\n",
              "      <td>55260.0</td>\n",
              "      <td>document</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350588</th>\n",
              "      <td>55260.0</td>\n",
              "      <td>Tel</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350589</th>\n",
              "      <td>55260.0</td>\n",
              "      <td>end</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>350590 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sentence_idx          word     tag\n",
              "0                1.0        RECORD       O\n",
              "1                1.0       10/2006  B-fndg\n",
              "2                1.0            12  I-fndg\n",
              "3                1.0      CORONARY       O\n",
              "4                1.0        ARTERY       O\n",
              "...              ...           ...     ...\n",
              "350585       55259.0    outpatient       O\n",
              "350586       55259.0  cardiologist       O\n",
              "350587       55260.0      document       O\n",
              "350588       55260.0           Tel       O\n",
              "350589       55260.0           end       O\n",
              "\n",
              "[350590 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_StuUtlmatEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "c4163069-b55a-4249-e756-190ae9031c35"
      },
      "source": [
        "test"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>11/19/1994</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>12</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>DIS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>DISCHARGE</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>DATE</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56014</th>\n",
              "      <td>34794.0</td>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56015</th>\n",
              "      <td>34795.0</td>\n",
              "      <td>activity</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56016</th>\n",
              "      <td>34795.0</td>\n",
              "      <td>activity</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56017</th>\n",
              "      <td>34796.0</td>\n",
              "      <td>Signed</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56018</th>\n",
              "      <td>34802.0</td>\n",
              "      <td>TEXT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56019 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_idx        word tag\n",
              "0               1.0  11/19/1994   O\n",
              "1               1.0          12   O\n",
              "2               1.0         DIS   O\n",
              "3               1.0   DISCHARGE   O\n",
              "4               1.0        DATE   O\n",
              "...             ...         ...  ..\n",
              "56014       34794.0         two   O\n",
              "56015       34795.0    activity   O\n",
              "56016       34795.0    activity   O\n",
              "56017       34796.0      Signed   O\n",
              "56018       34802.0        TEXT   O\n",
              "\n",
              "[56019 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt2NQAfDa0Or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "4871fdb7-748f-459d-b95c-b0999c7ef48c"
      },
      "source": [
        "dev"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>ID</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Status</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Aspiration</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>pneumonia</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>PRESENT</td>\n",
              "      <td>B-fndg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56009</th>\n",
              "      <td>34793.0</td>\n",
              "      <td>November</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56010</th>\n",
              "      <td>34793.0</td>\n",
              "      <td>patient</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56011</th>\n",
              "      <td>34793.0</td>\n",
              "      <td>Percocet</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56012</th>\n",
              "      <td>34797.0</td>\n",
              "      <td>M.D</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56013</th>\n",
              "      <td>34802.0</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>56014 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_idx        word     tag\n",
              "0               1.0          ID       O\n",
              "1               1.0      Status       O\n",
              "2               1.0  Aspiration       O\n",
              "3               1.0   pneumonia       O\n",
              "4               2.0     PRESENT  B-fndg\n",
              "...             ...         ...     ...\n",
              "56009       34793.0    November       O\n",
              "56010       34793.0     patient       O\n",
              "56011       34793.0    Percocet       O\n",
              "56012       34797.0         M.D       O\n",
              "56013       34802.0        ROOT       O\n",
              "\n",
              "[56014 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io0fB_4-a6FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "train.to_csv('train.csv')\n",
        "train2.to_csv('train2.csv')\n",
        "dev.to_csv('dev.csv')\n",
        "test.to_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjjIpudyif8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_unstructured=pd.DataFrame({'Test Tag':test_labels,'Predicted Tag':pred_labels})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsG4F76Xioj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_unstructured.to_csv('result_unstructured.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMyQ2CT4gGu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tag_temp01=[[tag2idx_all[w[1]] for w in s] for s in sentences_test]\n",
        "test_tag_ind01=[]\n",
        "for i in range(len(test_tag_temp01)):\n",
        "  test_tag_ind01.append(len(test_tag_temp01[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zllFAHBclrtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_tag_temp01=[]\n",
        "for i in range(len(pred_labels)):\n",
        "  predicted_tag_temp01.extend(pred_labels[i][:test_tag_ind01[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8KC58rIoixV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result=test.assign(predicted_tag=predicted_tag_temp01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPPwedgupp9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.to_csv('result.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI3aXBlRrNA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}